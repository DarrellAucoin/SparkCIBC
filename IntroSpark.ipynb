{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<style>\n",
    "\n",
    "@font-face {\n",
    "    font-family: \"Computer Modern\";\n",
    "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
    "}\n",
    "#notebook_panel { /* main background */\n",
    "    background: #888;\n",
    "    color: #f6f6f6;\n",
    "}\n",
    "#notebook li { /* More space between bullet points */\n",
    "margin-top:0.8em;\n",
    "}\n",
    "div.text_cell_render{\n",
    "    font-family: 'Arvo' sans-serif;\n",
    "    line-height: 110%;\n",
    "    font-size: 135%;\n",
    "    width:1000px;\n",
    "    margin-left:auto;\n",
    "    margin-right:auto;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<p class=\"gap05\"<p>\n",
    "<h1>Introduction to</h1>\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"images/spark-logo.png\" alt=\"Spark Logo\" >\n",
    "\n",
    "\n",
    "</center>\n",
    "<p class=\"gap05\"<p>\n",
    "\n",
    "<p class=\"gap05\"<p>\n",
    "<center>\n",
    "<h3>Darrell Aucoin</h3>\n",
    "</center>\n",
    "<center>\n",
    "\n",
    "<h3>CIBC: Data Engineer Manager's WorkZone Project</h3>\n",
    "\n",
    "</center>\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".input_prompt, .input_area, .output_prompt \n",
    "</style>\n",
    "\n",
    "Source: Learning Spark - O'Reilly Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "There are several computational environments:  \n",
    "- __Shared CPU, RAM, and Harddrive__: Single core computers\n",
    "- __Shared RAM and Harddrive__: Modern multi-core computers\n",
    "- __Shared Harddrive__: Some server clusters (sharcnet)\n",
    "- __Nothing Shared__: Hadoop Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce Overview\n",
    "\n",
    "__Definition.__ _MapReduce_ is a programming paradigm model of using parallel, distributed algorithims to process or generate data sets. MapReduce is composed of two main functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Map(k,v)__: Filters and sorts data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Reduce(k,v)__: Aggregates data according to keys (k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "MapReduce is broken down into several steps:\n",
    "\n",
    "1. Record Reader\n",
    "2. Map\n",
    "3. Combiner (Optional)\n",
    "4. Partitioner\n",
    "5. Shuffle and Sort\n",
    "6. Reduce\n",
    "7. Output Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](images/MapReduce.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MapReduce Phases\n",
    "\n",
    "1. __Record Reader__ Translates an input into records to be processed by the user-defined map function in the form of a key-value pair on each map cluster. \n",
    "\n",
    "2. __Map__ _User defined function_ outputing intermediate key-value pairs for the reducers\n",
    "\n",
    "    - __key__ ($k_{2}$): Later, MapReduce will group and possibly aggregate data according to these keys, choosing the right keys is here is important for a good MapReduce job.\n",
    "\n",
    "    - __value__ ($v_{2}$): The data to be grouped according to it's keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. __Record Reader__\n",
    "2. __Map__\n",
    "3. __Combiner__ _User defined function_ that aggregates data according to intermediate keys on a mapper node\n",
    "    - This can usually reduce the amount of data to be sent over the network increasing efficiency\n",
    "    $$\\left.\\begin{array}{r}\n",
    "\\left(\\mbox{\"hello world\"},1\\right)\\\\\n",
    "\\left(\\mbox{\"hello world\"},1\\right)\\\\\n",
    "\\left(\\mbox{\"hello world\"},1\\right)\n",
    "\\end{array}\\right\\} \\overset{\\mbox{combiner}}{\\longrightarrow}\\left(\\mbox{\"hello world\"},3\\right) $$\n",
    "\n",
    "4. __Partitioner__ Sends intermediate key-value pairs (k,v) to reducer by\n",
    "    $$\\mbox{Reducer}=\\mbox{hash}\\left(\\mbox{k}\\right)\\pmod{R}$$  \n",
    "5. __Shuffle and Sort__ On reducer node, sorts by key to help group equivalent keys\n",
    "6. __Reduce__ _User Defined Function_ that aggregates data (v) according to keys (k) to send key-value pairs to output\n",
    "7. __Output Format__ Translates final key-value pairs to file format (tab-seperated by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](images/MapReduce.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark's DAG Model\n",
    "A more flexible form of MapReduce is used by Spark using __Directed Acyclic Graphs (DAG)__. \n",
    "![alt text](images/DAG.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a set of operations:  \n",
    "1. Create a DAG for operations\n",
    "2. Divide DAG into tasks\n",
    "3. Assign tasks to nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Overview\n",
    "Apache Spark is an alternative to the MapReduce in the Hadoop environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Where Hadoop MapReduce write to harddrive after every MapReduce operation, Spark never writes to harddrive until explicitly told to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Spark is more flexiable and faster than YARN (Hadoop MapReduce) allowing abstractions of what is to be done and letting the Spark engine optimize the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Popular Use Cases\n",
    "\n",
    "- Processing in parallel large data sets distributed across a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Performing ad hoc or interactive queries to explore and visualize data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Building, training, and evaluating machine learning models using MLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Implementing end-to-end data pipelines from myriad steams of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Analyzing graph data sets and social networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Modules\n",
    "Modules built on Spark:  \n",
    "- __Spark SQL and DataFrames/Datasets__: support for structured data and relational queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __Spark Streaming__: processing real-time data streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __MLlib__: built-in machine learning library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __GraphX__: Spark’s API for graph processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark SQL and DataFrames/Datasets\n",
    "\n",
    "__RDD (Resilient Distributed Dataset)__: RDDs are a fault-tolerant distributed collection of items distributed across many compute nodes and can be manipulated in parallel. \n",
    "\n",
    "- If a portion of the RDD fails then only a portion of RDD needs to be recalculated from previous RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Spark DataFrames__: Are built on top of RDDs and have commonly used optimized functions for data manipulation. These are designed for stuctured data like csv, JSON, and text data.\n",
    "\n",
    "- DataFrames and Datasets have their APIs unified in the version of Spark we are using, thus I will simply refer to them as DataFrames.\n",
    "\n",
    "- DataFrames can be named as a table to enable SQL to perform queries on them.\n",
    "\n",
    "```python\n",
    "table = spark.read.json(\"s3://..../filename.json\") \\\n",
    "        .createOrReplaceTempView(\"table_name\")\n",
    "results = spark.sql(\"select * from table_name\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Streaming\n",
    "Spark Streaming enables data to be continously added as new rows to a table, where developers can issue queries against like a regular DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "lines = (spark \n",
    "        .readStream \n",
    "        .format(\"socket\") \n",
    "        .option(\"host\", \"localhost\") \n",
    "        .option(\"port\", 9999) \n",
    "        .load())\n",
    "words = lines.select(explode(split(lines.value, \"\")).alias(\"word\"))\n",
    "word_counts = words.groupBy(\"word\").count()\n",
    "query = (word_counts\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"topic\", \"output\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine Learning (MLib)\n",
    "MLib is the Machine Learning component built within Spark. This enables various Machine Learning Models to be training on Big Data in a distributed setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "training = spark.read.csv(\"s3://.../filename_train.csv\")\n",
    "test = spark.read.csv(\"s3://.../filename_test.csv\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(training)\n",
    "lrModel.transform(test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Graph Processing GraphX\n",
    "GraphX is the Spark library for manipulating graphs (social network graphs, network topology graphs) and performing graph paralllel computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scala\n",
    "```scala\n",
    "val graph = Graph(vertices, edges)\n",
    "messages = spark.textFile(\"hdfs://filename\")\n",
    "val graph2 = graph.joinVertices(messages) {\n",
    "(id, vertex, msg) => ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Basic Concepts\n",
    "\n",
    "## Transformations and Actions\n",
    "\n",
    "There are two types of RDD operations: transformations and actions.\n",
    "\n",
    "__Transformations__ construct a new RDD from the previous one. (mapping, filtering, etc.)\n",
    "- A list of transformations is set onto a DataFrame to make a new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Actions__ compute a result based on an RDD and either return it to the driver program or save it to a file.\n",
    "- Spark constructs a Directed Acyclic Graph (DAG) based on transformations and only groups them into stages and tasks when an action is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Note__: Reminder that DataFrames are a type of RDDs and everything that can be said about RDDs applies to DataFrames as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __No computations or memory is used__ until an action is called (save to HDD, report back to user, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each time an action uses a RDD, it has to use all of the transformations before it. This is important when feeding a DataFrame into a Machine Learning Model\n",
    "    - Unless it's cached (saved into memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples of Transformations and Actions\n",
    "\n",
    "| Transformations | Actions   |\n",
    "|-----------------|-----------|\n",
    "| `orderBy()`       | `show()`    |\n",
    "| `groupBy()`       | `take()`    |\n",
    "| `filter()`        | `count()`   |\n",
    "| `select()`        | `collect()` |\n",
    "| `join()`          | `save()`    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distributed Execution Components\n",
    "\n",
    "__Spark Driver__: Instantiates the SparkSession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Communicates with the cluster manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Requests resources from cluster manager for executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Transforms all Spark operations into DAG computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Schedules operations and distributes thems as tasks on Spark executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__SparkSession__: Conduit for all Spark operations and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- creates JVM runtime parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- defines DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- reads from data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- accesses catalog metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- issues Spark SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Cluster Manager__: Manages and allocates resources for the cluster of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can use built-in standalone cluster manger, or Apache Hadoop, YARN, Apache Mesos, and Kubernets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Spark Executor__: Runs on each worker cluster node and executes task on said workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Communicates with the driver program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Terms\n",
    "\n",
    "__Application__: User program built on Spark using it's APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__SparkSession__: Point of entry to interact with Spark functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Job__: Parallel computation consisting of mutiple tasks that gets spawned in response to a spark action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Stage__: Each job gets divided into smaller sets of tasks called stages which can be operated within one worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Task__: Single unit of work or execution that will be sent to a Spark executor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Operation\n",
    "First a `SparkSession` is created and an application is sent to the Spark driver. The Spark driver then converts the application into one or more spark jobs, and each job into a DAG. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The DAG is then broken down into stages based on what operations can be performed serially or in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each Stage is composed of Spark tasks (units of execution). This is federated across the Spark exectors and each task maps to a single core and partition of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- These transformations does not alter the original data, rather than change the values in the DataFrame, a new DataFrame is created from the last one. They are also evaluated lazily (only executed after an action is issued).\n",
    "    - This is an important feature that creates fault tolerance and parallelization for Spark jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Narrow and Wide Transformations\n",
    "__Narrow transformations__ are transformations that are row independent. Data does not have to be exchanged over the network for this evaluation.\n",
    "\n",
    "- `filter()`, `contains()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Wide transformations__ in contrast require data exhanged over the network. This is because there is a dependancy of one row to another.\n",
    "\n",
    "- `groupBy()`, `orderBy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deployment Modes\n",
    "| Mode           | Spark driver                   | Spark executor                       | Cluster manager                                                                     |   |\n",
    "|----------------|--------------------------------|--------------------------------------|-------------------------------------------------------------------------------------|---|\n",
    "| local          | Single JVM                     | same JVM as driver                   | same host                                                                           |   |\n",
    "| Standalone     | Any node on the cluster        | JVM for each node                    | allocated ot any host in cluster                                                    |   |\n",
    "| YARN (client)  | on client, not part of cluster | YARN's NodeManger's container        | YARN's Resource Manager and Applicaiton Master allocates containers on NodeManagers |   |\n",
    "| YARN (cluster) | with YARN Application Master   | same as client                       | same a client                                                                       |   |\n",
    "| Kubernetes     | in Kubernetes pod              | each worker runs within it's own pod | Kubernetes Master                                                                   |   |\n",
    "\n",
    "- Uses data locality to minimize network bandwith "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initializing a SparkConcept\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Web UI\n",
    "\n",
    "When Spark is deployed, the Spark driver launchs a web UI, running by default on port 4040, where you can view metrics and details of it's operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Details like:\n",
    "- A list of scheduler stages and tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A summary of RDD sizes and memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Information on the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Information about he running executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- All the Spark SQL queries exectued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In local mode: http://localhost:4040\n",
    "\n",
    "• https://databricks.com/try to use databrick's Spark's community edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "Based around Stats Club presentation example I did at University of Waterloo: https://github.com/DarrellAucoin/IntroSQL/wiki\n",
    "\n",
    "The problem is we are trying to solve is to give a budget report including when Stats Club went over budget on it's events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>presenter</th>\n",
       "      <th>organizer</th>\n",
       "      <th>poster</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>location</th>\n",
       "      <th>budget</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Begining of Term</td>\n",
       "      <td>social</td>\n",
       "      <td>NaN</td>\n",
       "      <td>judith</td>\n",
       "      <td>judith</td>\n",
       "      <td>2015-01-28 19:00:00</td>\n",
       "      <td>2015-01-28 22:00:00</td>\n",
       "      <td>C &amp; D</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Come and play games with your fellow stats Clu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>End of Term</td>\n",
       "      <td>social</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dominick</td>\n",
       "      <td>dominick</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>End of Term social at a local Pub. A joint eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intro to Hadoop</td>\n",
       "      <td>educational</td>\n",
       "      <td>darrell</td>\n",
       "      <td>darrell</td>\n",
       "      <td>gilberto</td>\n",
       "      <td>2015-03-25 14:30:00</td>\n",
       "      <td>2015-03-25 16:00:00</td>\n",
       "      <td>M3-3103</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Hadoop is a distributed computing system desig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intro to SQL</td>\n",
       "      <td>educational</td>\n",
       "      <td>darrell</td>\n",
       "      <td>darrell</td>\n",
       "      <td>patrick</td>\n",
       "      <td>2015-02-05 18:00:00</td>\n",
       "      <td>2015-02-05 19:30:00</td>\n",
       "      <td>MC-3003</td>\n",
       "      <td>90.0</td>\n",
       "      <td>SQL is a relational database language and alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prof Talk: Machine Learning</td>\n",
       "      <td>educational</td>\n",
       "      <td>Chong Zhang</td>\n",
       "      <td>patrick</td>\n",
       "      <td>dominick</td>\n",
       "      <td>2015-03-03 15:00:00</td>\n",
       "      <td>2015-03-03 16:00:00</td>\n",
       "      <td>M3-2134</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Machine Learning and Data Mining: How We can C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Prof Talk 2</td>\n",
       "      <td>educational</td>\n",
       "      <td>NaN</td>\n",
       "      <td>judith</td>\n",
       "      <td>judith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Intro to SQL: Basic Queries</td>\n",
       "      <td>educational</td>\n",
       "      <td>darrell</td>\n",
       "      <td>darrell</td>\n",
       "      <td>darrell</td>\n",
       "      <td>2015-03-09 18:00:00</td>\n",
       "      <td>2015-03-09 19:30:00</td>\n",
       "      <td>MC-3003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>SQL is a relational database language and alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Intro to SQL: Advanced Queries</td>\n",
       "      <td>educational</td>\n",
       "      <td>darell</td>\n",
       "      <td>darrell</td>\n",
       "      <td>darrell</td>\n",
       "      <td>2015-03-12 18:00:00</td>\n",
       "      <td>2015-03-12 19:30:00</td>\n",
       "      <td>MC-3003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>SQL is a relational database language and alon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name         type    presenter organizer  \\\n",
       "0                Begining of Term       social          NaN    judith   \n",
       "1                     End of Term       social          NaN  dominick   \n",
       "2                 Intro to Hadoop  educational      darrell   darrell   \n",
       "3                    Intro to SQL  educational      darrell   darrell   \n",
       "4     Prof Talk: Machine Learning  educational  Chong Zhang   patrick   \n",
       "5                     Prof Talk 2  educational          NaN    judith   \n",
       "6     Intro to SQL: Basic Queries  educational      darrell   darrell   \n",
       "7  Intro to SQL: Advanced Queries  educational       darell   darrell   \n",
       "\n",
       "     poster           start_time             end_time location  budget  \\\n",
       "0    judith  2015-01-28 19:00:00  2015-01-28 22:00:00    C & D    90.0   \n",
       "1  dominick                  NaN                  NaN      NaN   160.0   \n",
       "2  gilberto  2015-03-25 14:30:00  2015-03-25 16:00:00  M3-3103    90.0   \n",
       "3   patrick  2015-02-05 18:00:00  2015-02-05 19:30:00  MC-3003    90.0   \n",
       "4  dominick  2015-03-03 15:00:00  2015-03-03 16:00:00  M3-2134    90.0   \n",
       "5    judith                  NaN                  NaN      NaN    90.0   \n",
       "6   darrell  2015-03-09 18:00:00  2015-03-09 19:30:00  MC-3003   100.0   \n",
       "7   darrell  2015-03-12 18:00:00  2015-03-12 19:30:00  MC-3003   100.0   \n",
       "\n",
       "                                         Description  \n",
       "0  Come and play games with your fellow stats Clu...  \n",
       "1  End of Term social at a local Pub. A joint eve...  \n",
       "2  Hadoop is a distributed computing system desig...  \n",
       "3  SQL is a relational database language and alon...  \n",
       "4  Machine Learning and Data Mining: How We can C...  \n",
       "5                                                NaN  \n",
       "6  SQL is a relational database language and alon...  \n",
       "7  SQL is a relational database language and alon...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"data/event.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>expense</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intro to SQL</td>\n",
       "      <td>pizza</td>\n",
       "      <td>87.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intro to SQL</td>\n",
       "      <td>pop</td>\n",
       "      <td>15.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Begining of Term</td>\n",
       "      <td>pop</td>\n",
       "      <td>13.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Begining of Term</td>\n",
       "      <td>pizza</td>\n",
       "      <td>45.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>End of Term</td>\n",
       "      <td>pop</td>\n",
       "      <td>23.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>End of Term</td>\n",
       "      <td>veggie platter</td>\n",
       "      <td>25.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>End of Term</td>\n",
       "      <td>fries</td>\n",
       "      <td>21.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Intro to Hadoop</td>\n",
       "      <td>coffee</td>\n",
       "      <td>23.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Intro to Hadoop</td>\n",
       "      <td>water</td>\n",
       "      <td>10.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Intro to Hadoop</td>\n",
       "      <td>donuts</td>\n",
       "      <td>53.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Intro to Hadoop</td>\n",
       "      <td>cookies</td>\n",
       "      <td>10.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Intro to SQL: Basic Queries</td>\n",
       "      <td>cookies</td>\n",
       "      <td>10.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Intro to SQL: Basic Queries</td>\n",
       "      <td>donuts</td>\n",
       "      <td>20.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Intro to SQL: Basic Queries</td>\n",
       "      <td>pop</td>\n",
       "      <td>21.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Intro to SQL: Basic Queries</td>\n",
       "      <td>water</td>\n",
       "      <td>10.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Prof Talk: Machine Learning</td>\n",
       "      <td>pop</td>\n",
       "      <td>20.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Prof Talk: Machine Learning</td>\n",
       "      <td>pizza</td>\n",
       "      <td>62.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Prof Talk 2</td>\n",
       "      <td>pizza</td>\n",
       "      <td>61.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Prof Talk 2</td>\n",
       "      <td>pop</td>\n",
       "      <td>15.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>End of Term</td>\n",
       "      <td>meals</td>\n",
       "      <td>90.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Intro to SQL: Advanced Queries</td>\n",
       "      <td>cookies</td>\n",
       "      <td>10.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Intro to SQL: Advanced Queries</td>\n",
       "      <td>donuts</td>\n",
       "      <td>20.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Intro to SQL: Advanced Queries</td>\n",
       "      <td>pop</td>\n",
       "      <td>21.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Intro to SQL: Advanced Queries</td>\n",
       "      <td>water</td>\n",
       "      <td>10.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             event         expense  price\n",
       "0                     Intro to SQL           pizza  87.43\n",
       "1                     Intro to SQL             pop  15.34\n",
       "2                 Begining of Term             pop  13.23\n",
       "3                 Begining of Term           pizza  45.34\n",
       "4                      End of Term             pop  23.23\n",
       "5                      End of Term  veggie platter  25.23\n",
       "6                      End of Term           fries  21.21\n",
       "7                  Intro to Hadoop          coffee  23.12\n",
       "8                  Intro to Hadoop           water  10.23\n",
       "9                  Intro to Hadoop          donuts  53.23\n",
       "10                 Intro to Hadoop         cookies  10.23\n",
       "11     Intro to SQL: Basic Queries         cookies  10.23\n",
       "12     Intro to SQL: Basic Queries          donuts  20.34\n",
       "13     Intro to SQL: Basic Queries             pop  21.54\n",
       "14     Intro to SQL: Basic Queries           water  10.52\n",
       "15     Prof Talk: Machine Learning             pop  20.31\n",
       "16     Prof Talk: Machine Learning           pizza  62.56\n",
       "17                     Prof Talk 2           pizza  61.56\n",
       "18                     Prof Talk 2             pop  15.65\n",
       "19                     End of Term           meals  90.98\n",
       "20  Intro to SQL: Advanced Queries         cookies  10.23\n",
       "21  Intro to SQL: Advanced Queries          donuts  20.34\n",
       "22  Intro to SQL: Advanced Queries             pop  21.54\n",
       "23  Intro to SQL: Advanced Queries           water  10.52"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/expenses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First start up the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DecimalType, TimestampType\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"sparkExample\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we'll create the DataFrames for event and expenses. \n",
    "\n",
    "We will also create the schema as well. For this particular example we don't need to, however with the schema specified we don't need use up spark resources for infering the types and we can trouble shoot potential problems here before they become problems later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- presenter: string (nullable = true)\n",
      " |-- organizer: string (nullable = true)\n",
      " |-- poster: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- budget: decimal(10,2) (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eventSchema = StructType([     \n",
    "    StructField(\"name\",StringType(),False),     \n",
    "    StructField(\"type\",StringType(),False), \n",
    "    StructField(\"presenter\",StringType(),True), \n",
    "    StructField(\"organizer\",StringType(),True), \n",
    "    StructField(\"poster\",StringType(),True), \n",
    "    StructField(\"start_time\",TimestampType(),True), \n",
    "    StructField(\"end_time\",TimestampType(),True), \n",
    "    StructField(\"location\",StringType(),True),  \n",
    "    StructField(\"budget\",DecimalType(precision=10, scale=2),True), \n",
    "    StructField(\"description\",StringType(),True)])\n",
    "\n",
    "eventDF = spark.read.csv(\"data/event.csv\", header = True, schema = eventSchema, \n",
    "                         timestampFormat = \"yyyy/MM/dd HH:mm:ss\")\n",
    "eventDF.createOrReplaceTempView(\"event\")\n",
    "eventDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event: string (nullable = true)\n",
      " |-- expense: string (nullable = true)\n",
      " |-- price: decimal(10,2) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expensesSchema = StructType([ \n",
    "    StructField(\"event\",StringType(),False), \n",
    "    StructField(\"expense\",StringType(),False), \n",
    "    StructField(\"price\",DecimalType(precision=10, scale=2),False)])\n",
    "\n",
    "expensesDF = spark.read.csv(\"data/expenses.csv\", header = True, schema = expensesSchema) \n",
    "expensesDF.createOrReplaceTempView(\"expenses\")\n",
    "expensesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To solve this we need to sum up all the expenses for each event and then join this with the events table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "expenses = expensesDF.groupBy(\"event\").sum(\"price\") \\\n",
    "            .withColumnRenamed(\"sum(price)\", \"expense\") \n",
    "event = eventDF.select(\"name\", \"type\", \"budget\")\n",
    "budget_report = expenses.join(event, expenses.event == event.name, \"leftouter\") \\\n",
    "                .select(\"event\", \"type\", \"budget\", \"expense\") \\\n",
    "                .withColumn(\"warning\", \n",
    "                            expr(\"\"\"CASE WHEN budget - expense < 0 THEN 'Over budget'\n",
    "                            ELSE NULL END\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------+-------+-----------+\n",
      "|               event|       type|budget|expense|    warning|\n",
      "+--------------------+-----------+------+-------+-----------+\n",
      "|     Intro to Hadoop|educational| 90.00|  96.81|Over budget|\n",
      "|Intro to SQL: Bas...|educational|100.00|  62.63|       null|\n",
      "|         End of Term|     social|160.00| 160.65|Over budget|\n",
      "|         Prof Talk 2|educational| 90.00|  77.21|       null|\n",
      "|Intro to SQL: Adv...|educational|100.00|  62.63|       null|\n",
      "|    Begining of Term|     social| 90.00|  58.57|       null|\n",
      "|Prof Talk: Machin...|educational| 90.00|  82.87|       null|\n",
      "|        Intro to SQL|educational| 90.00| 102.77|Over budget|\n",
      "+--------------------+-----------+------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "budget_report.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- budget: decimal(10,2) (nullable = true)\n",
      " |-- expense: decimal(20,2) (nullable = true)\n",
      " |-- warning: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "budget_report.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Since we created a view of the original tables (`eventDF.createOrReplaceTempView(\"event\")`, `expensesDF.createOrReplaceTempView(\"expenses\")`) we can also pass a text SQL query through `spark.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------+--------+-----------+\n",
      "|                name|       type|budget|expenses|    warning|\n",
      "+--------------------+-----------+------+--------+-----------+\n",
      "|     Intro to Hadoop|educational| 90.00|   96.81|Over budget|\n",
      "|Intro to SQL: Bas...|educational|100.00|   62.63|       null|\n",
      "|         End of Term|     social|160.00|  160.65|Over budget|\n",
      "|         Prof Talk 2|educational| 90.00|   77.21|       null|\n",
      "|Intro to SQL: Adv...|educational|100.00|   62.63|       null|\n",
      "|    Begining of Term|     social| 90.00|   58.57|       null|\n",
      "|Prof Talk: Machin...|educational| 90.00|   82.87|       null|\n",
      "|        Intro to SQL|educational| 90.00|  102.77|Over budget|\n",
      "+--------------------+-----------+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"WITH cost (event, expenses) AS\n",
    "                    (SELECT event, SUM(price) AS expenses\n",
    "                    FROM expenses\n",
    "                    GROUP BY event)\n",
    "                SELECT e.name, e.type, e.budget, cost.expenses,\n",
    "                    CASE\n",
    "                    WHEN e.budget - cost.expenses < 0 THEN 'Over budget'\n",
    "                    ELSE NULL\n",
    "                    END AS warning\n",
    "                FROM event AS e RIGHT OUTER JOIN cost ON e.name = cost.event\"\"\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When everything is done, be nice to your database administrator and close the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#sudo jupyter nbconvert IntroSpark.ipynb --to slides --post serve"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
